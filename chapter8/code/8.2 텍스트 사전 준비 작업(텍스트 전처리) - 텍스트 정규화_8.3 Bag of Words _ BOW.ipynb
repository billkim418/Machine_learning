{"cells":[{"cell_type":"markdown","metadata":{"id":"4gyTEukxHLu-"},"source":["## 8.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화"]},{"cell_type":"markdown","metadata":{"id":"eMcccGg-HLvF"},"source":["### Text Tokenization"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdiNPh-8HLvG","executionInfo":{"status":"ok","timestamp":1650433947824,"user_tz":-540,"elapsed":364,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"9289b637-2818-4542-a7ee-6486388d1784"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","<class 'list'> 3\n","['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"]}],"source":["from nltk import sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n","               You can see it out your window or on your television. \\\n","               You feel it when you go to work, or go to church or pay your taxes.'\n","sentences = sent_tokenize(text=text_sample)\n","print(type(sentences),len(sentences))\n","print(sentences)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ldfsxmUHLvJ","executionInfo":{"status":"ok","timestamp":1650433948259,"user_tz":-540,"elapsed":48,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"64558779-d184-4204-e57c-f202f4703a44"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'> 15\n","['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"]}],"source":["from nltk import word_tokenize\n","\n","sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n","words = word_tokenize(sentence)\n","print(type(words), len(words))\n","print(words)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKysLSKcHLvL","executionInfo":{"status":"ok","timestamp":1650433948260,"user_tz":-540,"elapsed":45,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"c818890c-288f-4424-e569-4ba6f5c7a3cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'> 3\n","[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"]}],"source":["from nltk import word_tokenize, sent_tokenize\n","\n","#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n","def tokenize_text(text):\n","    \n","    # 문장별로 분리 토큰\n","    sentences = sent_tokenize(text)\n","    # 분리된 문장별 단어 토큰화\n","    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n","    return word_tokens\n","\n","#여러 문장들에 대해 문장별 단어 토큰화 수행. \n","word_tokens = tokenize_text(text_sample)\n","print(type(word_tokens),len(word_tokens))\n","print(word_tokens)"]},{"cell_type":"markdown","metadata":{"id":"VZAjS4o1HLvM"},"source":["### Stopwords 제거"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPdULVa3HLvN","executionInfo":{"status":"ok","timestamp":1650433948260,"user_tz":-540,"elapsed":40,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"5603a675-8c80-4d26-ab85-14329cbeadb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSH7ywkjHLvO","executionInfo":{"status":"ok","timestamp":1650433948261,"user_tz":-540,"elapsed":32,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"075c1016-9e12-467c-e144-7c77352fc201"},"outputs":[{"output_type":"stream","name":"stdout","text":["영어 stop words 갯수: 179\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"]}],"source":["print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n","print(nltk.corpus.stopwords.words('english')[:20])"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SouzXNk6HLvP","executionInfo":{"status":"ok","timestamp":1650433948262,"user_tz":-540,"elapsed":29,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"c7578c3f-ab42-4431-d6ec-26900ccc7256"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"]}],"source":["import nltk\n","\n","stopwords = nltk.corpus.stopwords.words('english')\n","all_tokens = []\n","# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n","for sentence in word_tokens:\n","    filtered_words=[]\n","    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n","    for word in sentence:\n","        #소문자로 모두 변환합니다. \n","        word = word.lower()\n","        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n","        if word not in stopwords:\n","            filtered_words.append(word)\n","    all_tokens.append(filtered_words)\n","    \n","print(all_tokens)"]},{"cell_type":"markdown","metadata":{"id":"4YUSsxvtHLvR"},"source":["### Stemming과 Lemmatization"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1Hbtrq6HLvR","executionInfo":{"status":"ok","timestamp":1650433948262,"user_tz":-540,"elapsed":23,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"16f3f7b5-26c6-4f19-c1d3-1167af3cc908"},"outputs":[{"output_type":"stream","name":"stdout","text":["work work work\n","amus amus amus\n","happy happiest\n","fant fanciest\n"]}],"source":["from nltk.stem import LancasterStemmer\n","stemmer = LancasterStemmer()\n","\n","print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n","print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n","print(stemmer.stem('happier'),stemmer.stem('happiest'))\n","print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ccvMxRFHLvS","executionInfo":{"status":"ok","timestamp":1650433950354,"user_tz":-540,"elapsed":2105,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"5764b523-681e-4383-c60c-910425ed5288"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","amuse amuse amuse\n","happy happy\n","fancy fancy\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('wordnet')\n","\n","lemma = WordNetLemmatizer()\n","print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n","print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n","print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"]},{"cell_type":"markdown","metadata":{"id":"UpF-7TP1HLvT"},"source":["## 8.3 Bag of Words – BOW\n","- 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델(문장내에서 몇 번의 단어가 나타내는지)\n","- 단점 : 1. 문맥 의미 반영 부족 2. 희소 행렬 문제\n"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"z8ZRba6pRYD9"}},{"cell_type":"markdown","metadata":{"id":"fxsnVAawHLvT"},"source":["### 희소 행렬 - COO 형식"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"qg5DXQdvHLvU","executionInfo":{"status":"ok","timestamp":1650433950354,"user_tz":-540,"elapsed":24,"user":{"displayName":"김홍범","userId":"10360439655098964254"}}},"outputs":[],"source":["import numpy as np\n","\n","dense = np.array( [ [ 3, 0, 1 ], [0, 2, 0 ] ] )"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"O-FuU_gPHLvU","executionInfo":{"status":"ok","timestamp":1650433950355,"user_tz":-540,"elapsed":22,"user":{"displayName":"김홍범","userId":"10360439655098964254"}}},"outputs":[],"source":["from scipy import sparse\n","\n","# 0 이 아닌 데이터 추출\n","data = np.array([3,1,2])\n","\n","# 행 위치와 열 위치를 각각 array로 생성 \n","row_pos = np.array([0,0,1])\n","col_pos = np.array([0,2,1])\n","\n","# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n","sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2CyNSPcHLvV","executionInfo":{"status":"ok","timestamp":1650433950356,"user_tz":-540,"elapsed":21,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"608a84b9-05b8-44eb-8d55-64ce21d3fb09"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[3, 0, 1],\n","       [0, 2, 0]])"]},"metadata":{},"execution_count":14}],"source":["sparse_coo.toarray()"]},{"cell_type":"markdown","metadata":{"id":"UocGU_dhHLvW"},"source":["### 희소 행렬 – CSR 형식"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QaZcoqONHLvW","executionInfo":{"status":"ok","timestamp":1650433950357,"user_tz":-540,"elapsed":20,"user":{"displayName":"김홍범","userId":"10360439655098964254"}},"outputId":"de17509e-8c1e-40d3-8f3b-fe69d7a5893e"},"outputs":[{"output_type":"stream","name":"stdout","text":["COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n","[[0 0 1 0 0 5]\n"," [1 4 0 3 2 5]\n"," [0 6 0 3 0 0]\n"," [2 0 0 0 0 0]\n"," [0 0 0 7 0 8]\n"," [1 0 0 0 0 0]]\n","CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n","[[0 0 1 0 0 5]\n"," [1 4 0 3 2 5]\n"," [0 6 0 3 0 0]\n"," [2 0 0 0 0 0]\n"," [0 0 0 7 0 8]\n"," [1 0 0 0 0 0]]\n"]}],"source":["from scipy import sparse\n","\n","dense2 = np.array([[0,0,1,0,0,5],\n","             [1,4,0,3,2,5],\n","             [0,6,0,3,0,0],\n","             [2,0,0,0,0,0],\n","             [0,0,0,7,0,8],\n","             [1,0,0,0,0,0]])\n","\n","# 0 이 아닌 데이터 추출\n","data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n","\n","# 행 위치와 열 위치를 각각 array로 생성 \n","row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n","col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n","\n","# COO 형식으로 변환 \n","sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n","\n","# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n","row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n","\n","# CSR 형식으로 변환 \n","sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n","\n","print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n","print(sparse_coo.toarray())\n","print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n","print(sparse_csr.toarray())\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"8fKo5gM7HLvX","executionInfo":{"status":"ok","timestamp":1650433950360,"user_tz":-540,"elapsed":20,"user":{"displayName":"김홍범","userId":"10360439655098964254"}}},"outputs":[],"source":["dense3 = np.array([[0,0,1,0,0,5],\n","             [1,4,0,3,2,5],\n","             [0,6,0,3,0,0],\n","             [2,0,0,0,0,0],\n","             [0,0,0,7,0,8],\n","             [1,0,0,0,0,0]])\n","\n","coo = sparse.coo_matrix(dense3)\n","csr = sparse.csr_matrix(dense3)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"oQJpAwK6HLvX","executionInfo":{"status":"ok","timestamp":1650433950362,"user_tz":-540,"elapsed":21,"user":{"displayName":"김홍범","userId":"10360439655098964254"}}},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"8.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화_8.3 Bag of Words _ BOW.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}